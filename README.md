# Capstone-Github-Portfolio
This repository presents my work in my capstone class. Our group did our group project on Kaggle's Home Credit Default Project

## Business Problem and Objective
Home Credit is a financial institution that provides loans for people with little or no credit history. Since many of their customers have this problem, most of their customers would be rejected at other places. This is a problem because Home Credit doesn’t want to reject clients that can repay their loans. Thus, causing the company to lose potential profit and not be able to help people that need it. If Home Credit can come up with a viable solution, it can help reduce the risk of their clients defaulting on their loans. This not only will help their clients be more successful but will also lead to more profit for the company. This project will be a success if Home Credit is able to increase its number of customers, help more clients, and gain more revenue. Therefore, Home Credit's objective is to accurately assess their applicant's loan repayment capability so that that can extend loans to creditworthy indiciduals and decline those who are likely to default.

## My Groups Solution
Our approach to finding a solution was to clean the data and then run as many models as we could. We implemented 7 different models and they were the following: Light Gradient Boosting, KNN, Random Forest, Gradient Boosting, Naive Bayes, Logistic Regression, and an Ensemble model using the Random Forest, Gradient Boosting, and Logistic Regression models. The model that gave us the best results was the Light Gradient Boosting model. We had two versions of this model, the first being the one using only the cleaned data. The second version’s data was oversampled using smote, and then downsampled using the random under sampler. We needed to do this because the first version model was overfitting the data. This model had an accuracy of 99% using the training data and 100% using the validation data. It had a kaggle score of 0.671. The top 5 features of this model were days_id_publish, days_registration, days_last_phone_change, ext_source_2, and amt_annuity. Our recommendation to Home credit is to use this model and to focus on those features of the applicants. We also found that married applicants with a higher education, who are businessmen/women, own a car, and own a house or an apartment are the least likely to default. Therefore, we recommend that Home Credit target this type of applicant when deciding who to offer a loan. We believe Home Credit can increase their revenue instantly if they follow this recommendation and implement our model when observing potential clients.

## My Contribution
For the EDA portion of the assignment, I looked into joining the dataset with the bureau dataset. In my analysis, I observed which numerical features had the highest correlation to the target variable. For categorical variables, I created boxplots to show how the data was distributed between the different target variables. For the modeling portion, I cleaned the data and then implemented the KNN, Light Gradient Boosting, and Gradient Boosting models. I also implemented the oversampling and then undersampling method on each of those three models. In the presentation, I handled the modeling and top features portion. 

## Our Group's Difficulties
I’d say our group’s biggest struggle was communicating with each other and procrastinating. We would take a little while to decide the plan for each portion of the project. However, once we did, we worked well together and did well with our project. My biggest struggle was finding a model that had good results and a good Kaggle score. It took me a while to implement the oversampling and undersampling model. However, once I figured it out, I was happy with the results and really proud of myself and our group.

## What I Learned
First of all, I am not an experienced user of Python. Therefore, I learned a lot about Python doing this project. I basically learned how to perform EDA, data cleaning, and modeling using Python for the first time. I also learned that it is important to have good communication when doing a team project. There were a lot of stressful moments that could have been avoided if we were better at this. The biggest lesson I learned is to never give up when coding. I failed a lot, but those failures taught me how to be a better programmer. There were times I definitely wanted to give up, and if I did, I would have never gotten the results I ended up getting. My advice for myself in the future is to be a better communicator when doing a team project and to be patient with myself when coding. 
